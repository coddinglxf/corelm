{
  "name": "CoreLM",
  "tagline": "A Continuous Space Neural Network Language Model based on Theano",
  "body": "# CoreLM \r\n\r\nCoreLM is a flexible and reusable feed-forward neural network which can be used to train neural language models and joint models (Devlin et. al, 2014), and interface with popular SMT systems like [MOSES](http://www.statmt.org/moses/). It is implemented in Python using [Theano](http://deeplearning.net/software/theano/), which makes is easy-to-use and modify. \r\n\r\n## Features\r\n\r\n* Implementation of self-normalized log-likelihood (Devlin et. al, 2014)  and noise contrastive estimation (NCE) loss functions, to train fast neural language models.\r\n* Decoder Integration with MOSES using NeuralLM and BilingualLM feature functions in MOSES. Also, rescoring MOSES n-best lists using neural language models. \r\n* Efficient and optimized implementation using Theano, capable of using GPU support for faster training and decoding. \r\n* The neural network architecture is flexible. Multiple hidden layers and various activation function, multiple sets of features with different embeddings etc.\r\n* The training is also flexible, with layer specific and adjustable learning rates, using various cost functions like log-likelihood and NCE and regularizations (L1 and L2). \r\n* Preprocessing scripts for monolingual language modeling and bilingual language modeling. \r\n\r\n## Getting Started\r\n\r\n### Prerequisites\r\n* Python Version 2.7 \r\n* Theano (See [installation instructions](http://deeplearning.net/software/theano/install_ubuntu.html)) with CUDA support (to use GPU)\r\n\r\n### Installation\r\n1. Download and unzip CoreLM package in your local machine.  Alternatively, you can clone using GIT.\r\n\t```\r\n\tgit clone https://github.com/nusnlp/corelm /path/to/corelm\r\n\t```\r\n\r\n2. Add the CoreLM directory to PYTHONPATH environment variable. For bash users, add the following line to ~/.bashrc : \r\n```\r\nexport PYTHONPATH=\"${PYTHONPATH}:/path/to/corelm/\"\r\n```\r\n\r\n## Using CoreLM\r\n\r\n\r\n### Preprocessing\r\n\r\nThe preprocessing scripts can be found in [dlm/preprocess/](dlm/preprocess) directory. The following scripts are available. For detailed help, run the required script with `--help` option. \r\n\r\n* **monolingual.py** : This script preprocesses a text file for monolingual language modeling. The text file must contain one sentence per line.\r\n\r\n* **bilingual.py** : This script preprocesses sentence aligned parallel corpora for bilingual language modeling. \r\n\r\n* **features.py** : This script can be used for sequence labeling tasks. The input text file must have one sentence per line, and one per-word feature is accepted.  An example is shown below:\r\n```\r\nword1_feature1 word2_feature2 word3_feature3 ... wordN_featureN \r\n```\r\n* *convert_to_memmap.py** : Custom inputs can be converted to input. The input must be a text file, with each line representing a training instance. The words or features must be replaced by corresponding indices according to the vocabulary file supplied. The format is as shown below:\r\n```\r\nword_index_11 word_index_12  ... word_index_1M output_word_index_1\r\n...\r\n...\r\nword_index_N1 word_index_N2  ... word_index_NM output_word_index_N\r\n```\r\nwhere M is the number of input words and N is the number of training instances.\r\n\r\n\r\n### Training\r\nTraining the neural network is done using the **train.py** script. The script takes in a memory-memory mapped file which is generated by the pre-processing scripts. Use `--help` for detailed list of options. \r\n\r\n\r\n### Testing\r\nEvaluation of the neural network can be done using **test.py** script. It prints the perplexities and log-likelihood of the models on the test set. It optionally outputs the predicted labels. To predict lablels of custom test instances use the **classify.py** script. See --help for each script. \r\n\r\n### Integration with Moses\r\nIntegration of language and joint models trained using CoreLM is achieved by two methods, re-ranking n-best hypothesis and decoder integration.\r\n* **Re-ranking** : To perform re-ranking of SMT n-best lists (in Moses format) using CoreLM models, first the weight of the new feature is to be trained using the **dlm/reranker/train.py**. This can be done using MERT or PRO, which can be set using command-line options. After training the weights, the re-ranking can be done using [dlm/reranker/rerank.py](dlm/reranker/rerank.py). Refer to `--help` for these scripts for the list of options. \r\n\r\n* **Decoder Integration** : Currently, CoreLM uses the NPLM interface to Moses for integration. CoreLM models can be converted to NPLM format using **dlm/misc/corelm_to_nplm.py** script. This can be integrated using `NeuralLM` and `BilingualLM` feature functions in Moses (See [Moses documentation](http://www.statmt.org/moses/?n=FactoredTraining.BuildingLanguageModel)).\r\n\r\n\r\n\r\n\r\n\r\n## License\r\nThis project is licensed under the MIT license - see the [LICENSE.md](LICENSE.md) file for details\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}